\documentclass[12pt]{article}
\usepackage{nips15submit_e,times}
\usepackage{url,graphicx,tabularx,array,geometry,amsmath,amssymb,amsthm,lipsum,hyperref}

\newcommand{\unionf}{\bigcup_{n=1}^{\infty} \mathcal{F}_n}
\newcommand{\F}{\mathcal{F}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\Hi}{\mathcal{H}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\Aseq}{\{A_k\}_{k=1}^{\infty}}
\newcommand{\unionA}{\bigcup_{k=1}^{\infty} A_k}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}

\nipsfinalcopy

\begin{document}

\title{CS181P Final Project Proposal \#1}

\author{
Santi Santichaivekin, Jingyi Liu, Jacob Boerma, Lorraine Zhao\\
Harvey Mudd College and Claremont McKenna College\\
Claremont, California, USA\\
}

\maketitle

\section{Team}
The team includes Santi Santichaivekin,
Jingyi Liu,
Jacob Boerma, and
Lorraine Zhao.
The team name is Go Pika Pika!
The team logo is provided below.

\includegraphics[width=0.25\textwidth]{Surprised_Pikachu_HD.jpg}

\section{Project}

We will attempt the second project listed on the project ideas:
\begin{quote}
ML, Search, and Compression. It has been shown that ML is a form of search (we will see this during the course, for example in Montan\~ez (Chapter 3)), and also shown that ML can be viewed as a form of compression (as in minimum description length approaches). Can we show compression is a form of search, search a form of compression, search a form of ML, or compression a form of ML, to make a triangle of two-way equivalences? Or taking a subset of these equivalences, can we apply a set of results from one domain to the other, such as forming no free lunch theorems for compression?
\end{quote}
We hypothesize that all the three problems can be phrased as one another. If we prove the three-way equivalency among the three problems, then we can apply different theories from one field to another.
Also, this problem is applicable to the real world, where we can solve actual real world problems by directing them as either ML, search, or compression. 

\section{Initial Plan of Attack}

We will start with understanding the concepts of ML, data compression, and search individually, and then trying to prove the “No Free Lunch” theorem for data compression among other theorems of search that might be applied to data compression. We can then look for algorithms for data compression and obtain insights about how it might be framed as a machine learning problem and search.

\section{Separation of Work}

For now, we will each read the papers and get on the same level of understanding. Once we have a good understanding of ML, compression, and search, we might split up work based on sub-problems that appear. We have divided the readings to different team members, so that one person can focus on one or two readings, and then update the rest of the team members on what he/she learned.

\begin{itemize}
\item
Jacob will read about No Free Lunch on a high level, and will try to understand the Famine of Forte paper and how we might apply the same concepts to compression schemes.
\item
Santi will be trying to understand the No Free Lunch Theory and apply it to compression.
\item
Lorraine will go over her part of the readings and get a better understanding of Machine Learning and Searches, and try to link the two by establishing an equivalent relationship.
\item
Rose will be reading about data compression algorithms and see how to generalize them as search problems. 
\end{itemize}

\section{Literature Review}

We have found some papers and textbooks that will be useful for our research.
We will read the suggested readings for the next few lectures in class, as well as the following papers/articles.

Why ML Works
\url{http://www.cs.cmu.edu/~gmontane/montanez_dissertation.pdf} [Lorraine]

“Famine of Forte”
\url{https://arxiv.org/pdf/1609.08913.pdf} [Jacob]

“Probabilistic machine learning and artificial intelligence”
\url{https://www.nature.com/articles/nature14541} [Rose]

“Generalization as Search”
\url{http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.121.5764&rep=rep1&type=pdf} [Lorraine]

“Data Compression Explained” 
\url{http://mattmahoney.net/dc/dce.html#Section_13} [Rose]

No Free Lunch For Optimization
\url{https://ti.arc.nasa.gov/m/profile/dhw/papers/78.pdf} [Santi] [Jacob]	

\end{document}
